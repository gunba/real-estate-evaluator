{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toggles\n",
    "USE_FEATURE_ENGINEERING = True\n",
    "USE_TRANSFORMATION = False\n",
    "USE_SCALER = True\n",
    "USE_WEIGHTED_RMSE = True\n",
    "USE_GBR_BIAS_CORRECTION = True\n",
    "USE_SYNTHETIC_SAMPLING = False\n",
    "USE_RFR_FEATURE_SELECTION = False\n",
    "\n",
    "# Protected Columns\n",
    "exclude_columns = ['reiwa_price', 'identifier', 'reiwa_is_sold']\n",
    "\n",
    "\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import json\n",
    "import itertools\n",
    "from tqdm.notebook import tqdm\n",
    "from scipy.stats import boxcox\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import SelectFromModel, VarianceThreshold\n",
    "from scipy.special import inv_boxcox\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from collections.abc import MutableMapping\n",
    "from plyer import notification\n",
    "from IPython.display import display, Javascript\n",
    "from IPython.core.magic import register_line_magic, Magics, magics_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to send a notification\n",
    "def notify(title, message):\n",
    "    notification.notify(\n",
    "        title=title,\n",
    "        message=message,\n",
    "        app_name='Jupyter Notebook'\n",
    "    )\n",
    "\n",
    "# Display a notification when the cell completes execution\n",
    "def notify_when_done(title='Task Complete', message='The cell execution is complete.'):\n",
    "    display(Javascript('''\n",
    "        (async () => {\n",
    "            await new Promise(resolve => {\n",
    "                setTimeout(resolve, 100);\n",
    "            });\n",
    "            await google.colab.kernel.invokeFunction('notebook.notify_completion', [], {});\n",
    "        })();\n",
    "    '''))\n",
    "    get_ipython().kernel.register_magics(CompletionNotifier)\n",
    "\n",
    "@magics_class\n",
    "class CompletionNotifier(Magics):\n",
    "    @register_line_magic\n",
    "    def notify_completion(self, line):\n",
    "        notify('Task Complete', 'The cell execution is complete.')\n",
    "\n",
    "get_ipython().register_magics(CompletionNotifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to flatten nested dictionaries\n",
    "def flatten_dict(d, parent_key='', sep='_'):\n",
    "    items = []\n",
    "    for k, v in d.items():\n",
    "        new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n",
    "        if isinstance(v, MutableMapping):\n",
    "            items.extend(flatten_dict(v, new_key, sep=sep).items())\n",
    "        else:\n",
    "            items.append((new_key, v))\n",
    "    return dict(items)\n",
    "\n",
    "# Load and process property data\n",
    "with open('property_data.json', 'r') as file:\n",
    "    property_data = json.load(file)\n",
    "property_df = pd.DataFrame(property_data)\n",
    "property_df.fillna(0, inplace=True)\n",
    "property_df['identifier'] = range(1, len(property_df) + 1)\n",
    "\n",
    "# Load and process suburb data\n",
    "with open('suburb_data.json', 'r') as file:\n",
    "    suburb_data = json.load(file)\n",
    "required_suburbs = set(property_df['reiwa_suburb'])\n",
    "filtered_suburb_data = {k: flatten_dict(v) for k, v in suburb_data.items() if k in required_suburbs}\n",
    "suburb_df = pd.DataFrame.from_dict(filtered_suburb_data, orient='index').reset_index().rename(columns={'index': 'reiwa_suburb'})\n",
    "suburb_df.fillna(0, inplace=True)\n",
    "\n",
    "# Merge property and suburb data\n",
    "df = pd.merge(property_df, suburb_df, on='reiwa_suburb', how='left')\n",
    "\n",
    "# Clean and filter the merged dataframe\n",
    "df = (\n",
    "    df.drop_duplicates(subset='reiwa_listing_id', keep='first')\n",
    "    .query('reiwa_suburb_interest_level.notnull()')\n",
    "    .assign(reiwa_price=lambda x: x[['reiwa_listing_price', 'reiwa_price']].max(axis=1))\n",
    "    .drop(columns=['reiwa_listing_price'])\n",
    "    .query('osm_local_community_population != 0')\n",
    ")\n",
    "\n",
    "# Create total rooms count for various filtering tasks\n",
    "df['total_rooms'] = df['reiwa_bedrooms'] + df['reiwa_bathrooms'] + df['reiwa_parking']\n",
    "\n",
    "# Identify houses under 40 sqm and scale up their land area if type == land or rooms > 4 (because the realtor meant hectares)\n",
    "df.loc[(df['reiwa_landsize'] < 40) & ((df['reiwa_house_type'] == 'Land') | (df['total_rooms'] > 4)), 'reiwa_landsize'] *= 10000\n",
    "\n",
    "# Sanity check to filter out unrealistic properties\n",
    "min_price_per_sqm = 1000\n",
    "max_price_per_sqm = 100000\n",
    "conditions = (\n",
    "    (df['reiwa_price'] <= 10000000) &\n",
    "    (df['reiwa_landsize'] > 25) & (df['reiwa_landsize'] <= 10000) &\n",
    "    ((df['total_rooms'] == 0) | (df['reiwa_landsize'] / df['total_rooms'] >= 10)) &\n",
    "    ((df['reiwa_bedrooms'] == 0) | (df['reiwa_price'] / df['reiwa_bedrooms'] >= 50000)) &\n",
    "    ((df['total_rooms'] > 0) | ((df['reiwa_price'] / df['reiwa_landsize'] >= min_price_per_sqm) & \n",
    "                                (df['reiwa_price'] / df['reiwa_landsize'] <= max_price_per_sqm)))\n",
    ")\n",
    "df = df[conditions].drop(columns=['total_rooms'])\n",
    "\n",
    "# Make 'identifier' the first column\n",
    "cols = ['identifier'] + [col for col in df.columns if col != 'identifier']\n",
    "df = df[cols]\n",
    "\n",
    "# Create a copy of the unsold houses with identifier saved\n",
    "df_target = df[df['reiwa_is_sold'] == False].copy()\n",
    "\n",
    "# Fill NaNs with zero (we have many optional columns)\n",
    "df = df.apply(lambda col: col.fillna(col.median()) if col.dtype != 'O' else col)\n",
    "\n",
    "# One-hot encode categorical fields\n",
    "df = pd.get_dummies(df, columns=['reiwa_agency_no', 'reiwa_suburb', 'reiwa_house_type', 'reiwa_local_government'])\n",
    "\n",
    "# Drop rows where 'suburb_interest_level' is null (indicates no valid REIWA data)\n",
    "df = df[df['reiwa_suburb_interest_level'].notnull()]\n",
    "\n",
    "# Drop text-based fields that are not being encoded\n",
    "text_fields = ['reiwa_address', 'reiwa_image_url', 'reiwa_details_url', 'reiwa_agency_name', 'scsa_school']\n",
    "df = df.drop(columns=text_fields)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_FEATURE_ENGINEERING:\n",
    "    # Create a ratio for crime against people vs. property\n",
    "    df['crime_ratio'] = np.where((df['wapol_total_person_crime'] != 0) & (df['wapol_total_property_crime'] != 0), df['wapol_total_property_crime'] / df['wapol_total_person_crime'], 0)\n",
    "\n",
    "    # Summarize total crime\n",
    "    df['crime_per_capita'] = (df['wapol_total_property_crime'] + df['wapol_total_person_crime']) / df['abs_people']\n",
    "\n",
    "    # Create affordability ratio for mortgages\n",
    "    df['affordability_ratio'] = df['abs_median_monthly_mortgage_repayment'] / df['abs_median_weekly_household_income']\n",
    "\n",
    "    # Convert local_dining and local_shop to per capita values\n",
    "    df['dining_per_capita'] = df['osm_local_dining'] / df['osm_local_community_population']\n",
    "    df['shop_per_capita'] = df['osm_local_shop'] / df['osm_local_community_population']\n",
    "\n",
    "    # Interaction Features\n",
    "    df['landsize_population_interaction'] = df['reiwa_landsize'] * df['osm_local_community_population']\n",
    "    df['rooms_bathrooms_interaction'] = df['reiwa_bedrooms'] * df['reiwa_bathrooms']\n",
    "\n",
    "    # Polynomial Features\n",
    "    df['landsize_squared'] = df['reiwa_landsize'] ** 2\n",
    "    df['distance_to_perth_cbd_squared'] = df['osm_distance_to_perth_cbd'] ** 2\n",
    "\n",
    "    # Distance Ratios\n",
    "    df['distance_airport_cbd_ratio'] = df['osm_distance_to_perth_airport'] / (df['osm_distance_to_perth_cbd'] + 1)\n",
    "    df['distance_fuel_station_ratio'] = df['osm_nearest_fuel_station'] / (df['osm_distance_to_perth_cbd'] + 1)\n",
    "\n",
    "    # Ratio of median weekly rent to median weekly household income\n",
    "    df['rent_income_ratio'] = df['abs_median_weekly_rent'] / (df['abs_median_weekly_household_income'] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#garbage_fields = ['reiwa_pets_allowed', 'abs_female_ratio', 'reiwa_floor_plan_count']\n",
    "#df = df.drop(columns=garbage_fields)\n",
    "def drop_low_variance_bools(df, threshold=10):\n",
    "    \"\"\"\n",
    "    Drop integer columns that only contain values of 0 and 1 and have very few 1 records.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input dataframe.\n",
    "    threshold (int): The maximum number of '1' values allowed for the column to be dropped. Default is 10.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The dataframe with the specified columns dropped.\n",
    "    \"\"\"\n",
    "    cols_to_drop = []\n",
    "    for col in df.select_dtypes(include='bool').columns:\n",
    "        if set(df[col].unique()).issubset({0, 1}):\n",
    "            if df[col].sum() < threshold:\n",
    "                cols_to_drop.append(col)\n",
    "    \n",
    "    df_dropped = df.drop(columns=cols_to_drop)\n",
    "    return df_dropped\n",
    "\n",
    "#df = drop_low_variance_bools(df, threshold=10)\n",
    "def drop_columns_by_prefix(df, prefixes):\n",
    "    \"\"\"\n",
    "    Drop columns that begin with any of the specified prefixes.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input dataframe.\n",
    "    prefixes (list of str): The list of prefixes to check.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The dataframe with the specified columns dropped.\n",
    "    \"\"\"\n",
    "    cols_to_drop = [col for col in df.columns if any(col.startswith(prefix) for prefix in prefixes)]\n",
    "    df_dropped = df.drop(columns=cols_to_drop)\n",
    "    return df_dropped\n",
    "\n",
    "prefixes = [\"wapol_offences_\"]\n",
    "df = drop_columns_by_prefix(df, prefixes)\n",
    "\n",
    "# Log our target variable (must take place before scaler)\n",
    "df['reiwa_price'], boxcox_lambda = boxcox(df['reiwa_price'])\n",
    "\n",
    "# Function to determine if a column should use square root transformation\n",
    "def should_use_sqrt(col):\n",
    "    unique_values = df[col].nunique()\n",
    "    return unique_values <= 20\n",
    "\n",
    "# Collect transformable columns\n",
    "transformable_columns = []\n",
    "\n",
    "for col in df.columns:\n",
    "    if col in exclude_columns or df[col].dtype == bool or set(df[col].unique()) == {0, 1}:\n",
    "        continue\n",
    "    \n",
    "    # Debug: Print the minimum value of the column\n",
    "    min_value = df[col].min()\n",
    "\n",
    "    transformable_columns.append(col)\n",
    "\n",
    "if USE_TRANSFORMATION:\n",
    "    for col in transformable_columns:\n",
    "        if (df[col] < 0).any():\n",
    "            df[col] = df[col] * -1\n",
    "\n",
    "        if should_use_sqrt(col):\n",
    "            df[col] = np.sqrt(df[col])\n",
    "        else:\n",
    "            try:\n",
    "                if (df[col] <= 0).any():\n",
    "                    df[col] += abs(df[col].min()) + 1  # Shift to make all values positive\n",
    "                df[col], _ = boxcox(df[col])\n",
    "            except ValueError as e:\n",
    "                continue\n",
    "\n",
    "# Apply scaling if the toggle is on\n",
    "if USE_SCALER:\n",
    "    # Initialize StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    # Apply StandardScaler only to the transformed columns\n",
    "    df[transformable_columns] = scaler.fit_transform(df[transformable_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection_with_rfr(df, target, protected_columns, variance_threshold=0.01, feature_importance_threshold=0.01, n_estimators=50, max_depth=5):\n",
    "    \"\"\"\n",
    "    Perform feature selection using Variance Threshold and Random Forest Regressor, retaining specified protected columns.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The dataframe containing the data.\n",
    "    target (str): The target column name.\n",
    "    protected_columns (list): A list of columns that should not be dropped.\n",
    "    variance_threshold (float): The threshold for variance to remove low-variance features.\n",
    "    feature_importance_threshold (float): The threshold for feature importance. Features with importance below this value will be dropped.\n",
    "    n_estimators (int): Number of trees in the forest.\n",
    "    max_depth (int): Maximum depth of the tree.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: The dataframe with unnecessary columns dropped.\n",
    "    \"\"\"\n",
    "    # Separate features and target\n",
    "    X = df.drop(columns=[target])\n",
    "    y = df[target]\n",
    "    \n",
    "    # Apply Variance Threshold to reduce dimensionality\n",
    "    vt = VarianceThreshold(threshold=variance_threshold)\n",
    "    X_reduced = vt.fit_transform(X)\n",
    "    \n",
    "    # Get the reduced feature names\n",
    "    reduced_features = X.columns[vt.get_support()]\n",
    "    \n",
    "    # Create a new dataframe with the reduced features\n",
    "    X_reduced_df = pd.DataFrame(X_reduced, columns=reduced_features, index=X.index)\n",
    "    \n",
    "    # Initialize and fit Random Forest Regressor\n",
    "    model = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth, random_state=42, n_jobs=-1, verbose=1)\n",
    "    model.fit(X_reduced_df, y)\n",
    "    \n",
    "    # Select features based on importance\n",
    "    selector = SelectFromModel(model, threshold=feature_importance_threshold, prefit=True)\n",
    "    selected_features = X_reduced_df.columns[(selector.get_support())]\n",
    "    \n",
    "    # Ensure protected columns are retained\n",
    "    columns_to_keep = set(selected_features).union(set(protected_columns))\n",
    "    \n",
    "    # Ensure all columns to keep are in the dataframe\n",
    "    columns_to_keep = [col for col in columns_to_keep if col in df.columns]\n",
    "    \n",
    "    # Drop unnecessary columns directly from the original dataframe\n",
    "    columns_to_drop = set(df.columns) - set(columns_to_keep) - {target}\n",
    "    df.drop(columns=columns_to_drop, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "if USE_RFR_FEATURE_SELECTION:\n",
    "    df = feature_selection_with_rfr(df, 'reiwa_price', exclude_columns, max_depth=None, n_estimators=200, feature_importance_threshold=0.005)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afa0e0a73f13424fa9363d35d5ab6693",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Assuming df is already loaded\n",
    "# We only want to train on is_sold == True since we are predicting pre-sales\n",
    "df_sold = df[df['reiwa_is_sold'] == True]\n",
    "\n",
    "# We modify this dataframe later so we copy it to avoid warnings\n",
    "df_unsold = df[df['reiwa_is_sold'] == False].copy()\n",
    "\n",
    "# Split the sold data into features (X) and target variable (y), excluding 'identifier'\n",
    "X = df_sold.drop(['reiwa_price', 'reiwa_is_sold', 'identifier'], axis=1)\n",
    "y = df_sold['reiwa_price']\n",
    "\n",
    "# Perform the train-test split first to avoid data leakage\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Prepare the unsold data for evaluation\n",
    "X_unsold = df_unsold.drop(['reiwa_price', 'reiwa_is_sold', 'identifier'], axis=1)\n",
    "y_unsold = df_unsold['reiwa_price']\n",
    "\n",
    "# Function to create synthetic samples by adding noise\n",
    "def create_synthetic_samples(X, y, n_samples, noise_level=0.01):\n",
    "    synthetic_X = np.tile(X, (n_samples, 1))\n",
    "    synthetic_y = np.tile(y, n_samples)\n",
    "    noise = np.random.normal(0, noise_level, synthetic_X.shape)\n",
    "    return synthetic_X + noise, synthetic_y\n",
    "\n",
    "# Function to generate synthetic data within specified ranges\n",
    "def generate_synthetic_data(X_train, y_train, value_ranges):\n",
    "    X_resampled = X_train.values\n",
    "    y_resampled = y_train.values\n",
    "\n",
    "    for value_range, n_synthetic_samples in value_ranges:\n",
    "        range_min, range_max = value_range\n",
    "        indices = (y_train >= range_min) & (y_train < range_max)\n",
    "        X_range = X_train[indices]\n",
    "        y_range = y_train[indices]\n",
    "\n",
    "        if len(y_range) > 0:\n",
    "            X_synthetic, y_synthetic = create_synthetic_samples(X_range.values, y_range.values, n_synthetic_samples)\n",
    "            X_resampled = np.vstack([X_resampled, X_synthetic])\n",
    "            y_resampled = np.hstack([y_resampled, y_synthetic])\n",
    "\n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "# Define the value ranges and the number of synthetic samples for each range\n",
    "value_ranges = [\n",
    "    ((0, 200000), 5),         # Low values # High values\n",
    "    ((3000000, 10000000), 5)    # Very high values\n",
    "]\n",
    "\n",
    "if USE_SYNTHETIC_SAMPLING:\n",
    "    X_resampled, y_resampled = generate_synthetic_data(X_train, y_train, value_ranges)\n",
    "else:\n",
    "    X_resampled = X_train.values\n",
    "    y_resampled = y_train.values\n",
    "\n",
    "# Create DMatrix for XGBoost\n",
    "dtrain = xgb.DMatrix(X_resampled, label=y_resampled, nthread=-1)\n",
    "dtest = xgb.DMatrix(X_test.values, label=y_test.values, nthread=-1)\n",
    "dunsold = xgb.DMatrix(X_unsold.values, label=y_unsold.values, nthread=-1)\n",
    "\n",
    "# Custom callback to update tqdm progress bar\n",
    "class TqdmCallback(xgb.callback.TrainingCallback):\n",
    "    def __init__(self, total_rounds, early_stopping_rounds):\n",
    "        self.pbar = tqdm(total=total_rounds, desc=\"Training Progress\")\n",
    "        self.early_stopping_rounds = early_stopping_rounds\n",
    "        self.best_score = float(\"inf\")\n",
    "        self.best_iteration = 0\n",
    "        self.stopping_counter = 0\n",
    "\n",
    "    def after_iteration(self, model, epoch, evals_log):\n",
    "        train_rmse = evals_log['train']['rmse'][-1]\n",
    "        eval_rmse = evals_log['eval']['rmse'][-1]\n",
    "        unsold_rmse = evals_log['unsold']['rmse'][-1]\n",
    "\n",
    "        self.pbar.set_postfix({\n",
    "            'train_rmse': f'{train_rmse:.5f}', \n",
    "            'eval_rmse': f'{eval_rmse:.5f}',\n",
    "            'unsold_rmse': f'{unsold_rmse:.5f}',\n",
    "            'best_iteration': self.best_iteration\n",
    "        })\n",
    "        self.pbar.update(1)\n",
    "\n",
    "        # Early stopping logic\n",
    "        if eval_rmse < self.best_score:\n",
    "            self.best_score = eval_rmse\n",
    "            self.best_iteration = epoch\n",
    "            self.stopping_counter = 0\n",
    "        else:\n",
    "            self.stopping_counter += 1\n",
    "\n",
    "        if self.stopping_counter >= self.early_stopping_rounds:\n",
    "            self.pbar.set_postfix_str(f'Early stopping at iteration {self.best_iteration} with best score {self.best_score}')\n",
    "            self.pbar.close()\n",
    "            return True  # Return True to stop training\n",
    "        return False\n",
    "    \n",
    "\n",
    "# Train the model with early stopping and progress bar\n",
    "total_rounds = 10000\n",
    "early_stopping_rounds = 10\n",
    "\n",
    "# Hyperparameter grid\n",
    "param_grid = {\n",
    "    'learning_rate': [0.1],\n",
    "    'max_depth': [9],\n",
    "    'min_child_weight': [1],\n",
    "    'gamma': [0],\n",
    "    'subsample': [1],\n",
    "    'colsample_bytree': [1],\n",
    "    'lambda': [0.1],\n",
    "    'alpha': [0.1]\n",
    "}\n",
    "\n",
    "evals = [(dtrain, 'train'), (dtest, 'eval'), (dunsold, 'unsold')]\n",
    "\n",
    "param_combinations = list(itertools.product(\n",
    "    param_grid['learning_rate'],\n",
    "    param_grid['max_depth'],\n",
    "    param_grid['min_child_weight'],\n",
    "    param_grid['gamma'],\n",
    "    param_grid['subsample'],\n",
    "    param_grid['colsample_bytree'],\n",
    "    param_grid['lambda'],\n",
    "    param_grid['alpha']\n",
    "))\n",
    "\n",
    "# Initialize variables to store the best model and lowest unsold RMSE\n",
    "best_model = None\n",
    "lowest_unsold_rmse = float('inf')\n",
    "\n",
    "# Initialize dictionaries to store RMSE values for each hyperparameter\n",
    "rmse_values = {param: {} for param in param_grid.keys()}\n",
    "\n",
    "# Loop through each combination of hyperparameters\n",
    "for params in param_combinations:\n",
    "    xgb_params = {\n",
    "        'tree_method': 'hist',\n",
    "        'eval_metric': 'rmse',\n",
    "        'device': 'cuda',\n",
    "        'learning_rate': params[0],\n",
    "        'max_depth': params[1],\n",
    "        'min_child_weight': params[2],\n",
    "        'gamma': params[3],\n",
    "        'subsample': params[4],\n",
    "        'colsample_bytree': params[5],\n",
    "        'lambda': params[6],\n",
    "        'alpha': params[7]\n",
    "    }\n",
    "\n",
    "    # Custom callback to update tqdm progress bar\n",
    "    class TqdmCallback(xgb.callback.TrainingCallback):\n",
    "        def __init__(self, total_rounds, early_stopping_rounds):\n",
    "            self.pbar = tqdm(total=total_rounds, desc=\"Training Progress\")\n",
    "            self.early_stopping_rounds = early_stopping_rounds\n",
    "            self.best_score = float(\"inf\")\n",
    "            self.best_iteration = 0\n",
    "            self.stopping_counter = 0\n",
    "\n",
    "        def after_iteration(self, model, epoch, evals_log):\n",
    "            train_rmse = evals_log['train']['rmse'][-1]\n",
    "            eval_rmse = evals_log['eval']['rmse'][-1]\n",
    "            unsold_rmse = evals_log['unsold']['rmse'][-1]\n",
    "\n",
    "            self.pbar.set_postfix({\n",
    "                'train_rmse': f'{train_rmse:.5f}', \n",
    "                'eval_rmse': f'{eval_rmse:.5f}',\n",
    "                'unsold_rmse': f'{unsold_rmse:.5f}',\n",
    "                'best_iteration': self.best_iteration\n",
    "            })\n",
    "            self.pbar.update(1)\n",
    "\n",
    "            # Early stopping logic\n",
    "            if eval_rmse < self.best_score:\n",
    "                self.best_score = eval_rmse\n",
    "                self.best_iteration = epoch\n",
    "                self.stopping_counter = 0\n",
    "            else:\n",
    "                self.stopping_counter += 1\n",
    "\n",
    "            if self.stopping_counter >= self.early_stopping_rounds:\n",
    "                self.pbar.set_postfix_str(f'Early stopping at iteration {self.best_iteration} with best score {self.best_score}')\n",
    "                self.pbar.close()\n",
    "                return True  # Return True to stop training\n",
    "            return False\n",
    "\n",
    "    tqdm_callback = TqdmCallback(total_rounds, early_stopping_rounds)\n",
    "\n",
    "    wrmse_low = np.percentile(y_train, 10)\n",
    "    wrmse_high = np.percentile(y_train, 90)\n",
    "    def weighted_rmse(preds, dtrain):\n",
    "        y_true = dtrain.get_label()\n",
    "        residuals = preds - y_true\n",
    "        weights = np.where((y_true <= wrmse_low) | (y_true >= wrmse_high), 2.5, 1)\n",
    "        weighted_residuals = weights * residuals\n",
    "        grad = 2 * weighted_residuals\n",
    "        hess = 2 * weights\n",
    "        return grad, hess\n",
    "\n",
    "    if USE_WEIGHTED_RMSE:\n",
    "        xgb_model = xgb.train(\n",
    "            params=xgb_params, \n",
    "            dtrain=dtrain, \n",
    "            num_boost_round=total_rounds, \n",
    "            evals=evals, \n",
    "            obj=weighted_rmse, \n",
    "            callbacks=[tqdm_callback, xgb.callback.EarlyStopping(rounds=early_stopping_rounds)], \n",
    "            verbose_eval=0\n",
    "        )\n",
    "    else:\n",
    "        xgb_params['objective'] = 'reg:squarederror'\n",
    "        xgb_model = xgb.train(\n",
    "            params=xgb_params, \n",
    "            dtrain=dtrain, \n",
    "            num_boost_round=total_rounds, \n",
    "            evals=evals, \n",
    "            callbacks=[tqdm_callback, xgb.callback.EarlyStopping(rounds=early_stopping_rounds)], \n",
    "            verbose_eval=0\n",
    "        )\n",
    "\n",
    "    # Close the progress bar properly after training\n",
    "    tqdm_callback.pbar.close()\n",
    "\n",
    "    # Calculate RMSE on unsold data\n",
    "    unsold_predictions = xgb_model.predict(dunsold)\n",
    "    unsold_rmse = np.sqrt(mean_squared_error(y_unsold, unsold_predictions))\n",
    "\n",
    "    # Store the RMSE for each individual hyperparameter\n",
    "    for param_name, param_value in zip(param_grid.keys(), params):\n",
    "        if param_value not in rmse_values[param_name]:\n",
    "            rmse_values[param_name][param_value] = []\n",
    "        rmse_values[param_name][param_value].append(unsold_rmse)\n",
    "\n",
    "    # Check if this model has the lowest unsold RMSE\n",
    "    if unsold_rmse < lowest_unsold_rmse:\n",
    "        lowest_unsold_rmse = unsold_rmse\n",
    "        best_model = xgb_model\n",
    "        best_params = xgb_params\n",
    "\n",
    "    print(f\"Params: {xgb_params}, Unsold RMSE: {unsold_rmse:.8f}\")\n",
    "\n",
    "# Output the best model and its parameters\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "print(f\"Lowest Unsold RMSE: {lowest_unsold_rmse:.8f}\")\n",
    "\n",
    "# Calculate and print average RMSE for each hyperparameter in condensed format\n",
    "for param_name, param_values in rmse_values.items():\n",
    "    for param_value, rmses in sorted(param_values.items()):\n",
    "        avg_rmse = np.mean(rmses)\n",
    "        print(f\"{param_name} - {param_value} - {avg_rmse:.8f}\")\n",
    "\n",
    "# Save the best XGBoost model\n",
    "best_model.save_model('xgb_model.json')\n",
    "\n",
    "# Print results for the best model on test data\n",
    "xgb_test_predictions = best_model.predict(dtest)\n",
    "xgb_mae = mean_absolute_error(y_test, xgb_test_predictions)\n",
    "xgb_rmse = np.sqrt(mean_squared_error(y_test, xgb_test_predictions))\n",
    "\n",
    "print(f\"XGBoost - MAE: {xgb_mae:.8}, RMSE: {xgb_rmse:.8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted XGBoost - MAE: 0.00030719, RMSE: 0.00041486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jorda\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "if USE_GBR_BIAS_CORRECTION:\n",
    "    # Calculate residuals on the resampled training data\n",
    "    xgb_train_predictions = xgb_model.predict(dtrain)\n",
    "    residuals_train = y_resampled - xgb_train_predictions\n",
    "\n",
    "    # Reshape the resampled training target variable to be used with GBR\n",
    "    y_resampled_reshaped = y_resampled.reshape(-1, 1)\n",
    "\n",
    "    # Train a gradient boosting regressor on the resampled training residuals\n",
    "    gbr = GradientBoostingRegressor(n_estimators=200, max_depth=5, random_state=42)\n",
    "    gbr.fit(y_resampled_reshaped, residuals_train)\n",
    "\n",
    "    # Predict residuals for the test data using the trained GBR\n",
    "    y_test_reshaped = y_test.values.reshape(-1, 1)\n",
    "    predicted_residuals_test = gbr.predict(y_test_reshaped)\n",
    "\n",
    "    # Adjust the initial test predictions\n",
    "    adjusted_predictions = xgb_test_predictions + predicted_residuals_test\n",
    "\n",
    "    # Evaluate the adjusted predictions\n",
    "    adjusted_mae = mean_absolute_error(y_test, adjusted_predictions)\n",
    "    adjusted_rmse = mean_squared_error(y_test, adjusted_predictions, squared=False)\n",
    "    print(f\"Adjusted XGBoost - MAE: {adjusted_mae:.8f}, RMSE: {adjusted_rmse:.8f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "XGBoostError",
     "evalue": "[23:30:55] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:1455: Check failed: learner_model_param_.num_feature >= p_fmat->Info().num_col_ (2225 vs. 2226) : Number of columns does not match number of features in booster.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m xgb_model\u001b[38;5;241m.\u001b[39mload_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxgb_model.json\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m dunsold \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mDMatrix(X_unsold)\n\u001b[1;32m----> 5\u001b[0m xgb_predictions \u001b[38;5;241m=\u001b[39m \u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdunsold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Adjust the predictions using the previously fitted gradient boosting regressor model\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m USE_GBR_BIAS_CORRECTION:\n",
      "File \u001b[1;32mc:\\Users\\jorda\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\core.py:2297\u001b[0m, in \u001b[0;36mBooster.predict\u001b[1;34m(self, data, output_margin, pred_leaf, pred_contribs, approx_contribs, pred_interactions, validate_features, training, iteration_range, strict_shape)\u001b[0m\n\u001b[0;32m   2295\u001b[0m shape \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mPOINTER(c_bst_ulong)()\n\u001b[0;32m   2296\u001b[0m dims \u001b[38;5;241m=\u001b[39m c_bst_ulong()\n\u001b[1;32m-> 2297\u001b[0m \u001b[43m_check_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2298\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGBoosterPredictFromDMatrix\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2299\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2300\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2301\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfrom_pystr_to_cstr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2302\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2303\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdims\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2304\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2305\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2306\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2307\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _prediction_output(shape, dims, preds, \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\jorda\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\core.py:282\u001b[0m, in \u001b[0;36m_check_call\u001b[1;34m(ret)\u001b[0m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check the return value of C API call\u001b[39;00m\n\u001b[0;32m    272\u001b[0m \n\u001b[0;32m    273\u001b[0m \u001b[38;5;124;03mThis function will raise exception when error occurs.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;124;03m    return value from API calls\u001b[39;00m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 282\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m XGBoostError(py_str(_LIB\u001b[38;5;241m.\u001b[39mXGBGetLastError()))\n",
      "\u001b[1;31mXGBoostError\u001b[0m: [23:30:55] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:1455: Check failed: learner_model_param_.num_feature >= p_fmat->Info().num_col_ (2225 vs. 2226) : Number of columns does not match number of features in booster."
     ]
    }
   ],
   "source": [
    "# Load the XGBoost model\n",
    "xgb_model = xgb.Booster()\n",
    "xgb_model.load_model('xgb_model.json')\n",
    "dunsold = xgb.DMatrix(X_unsold)\n",
    "xgb_predictions = xgb_model.predict(dunsold)\n",
    "\n",
    "# Adjust the predictions using the previously fitted gradient boosting regressor model\n",
    "if USE_GBR_BIAS_CORRECTION:\n",
    "    unsold_actual_prices = xgb_predictions.reshape(-1, 1)\n",
    "    unsold_predicted_residuals = gbr.predict(unsold_actual_prices)\n",
    "    adjusted_xgb_predictions = xgb_predictions + unsold_predicted_residuals\n",
    "else:\n",
    "    adjusted_xgb_predictions = xgb_predictions\n",
    "\n",
    "# Add adjusted predictions to the unsold data\n",
    "df_unsold.loc[:, 'model_prediction'] = inv_boxcox(adjusted_xgb_predictions, boxcox_lambda)\n",
    "\n",
    "# Merge the predictions with property_data\n",
    "df_target_pred = pd.merge(property_df, df_unsold[['identifier', 'model_prediction']], on='identifier', how='left')\n",
    "\n",
    "# Reorder columns to place price_prediction as the third column\n",
    "cols = list(df_target_pred.columns)\n",
    "prediction_index = cols.index('model_prediction')\n",
    "if prediction_index != 2:\n",
    "    cols.insert(2, cols.pop(prediction_index))\n",
    "df_target_pred = df_target_pred[cols]\n",
    "\n",
    "# Remove rows with no predicted price\n",
    "rows_before = df_target_pred.shape[0]\n",
    "df_target_cleaned = df_target_pred.dropna(subset=['model_prediction'])\n",
    "rows_after = df_target_cleaned.shape[0]\n",
    "\n",
    "# Print the number of rows removed\n",
    "rows_removed = rows_before - rows_after\n",
    "print(f\"Number of rows removed: {rows_removed}\")\n",
    "\n",
    "# Convert the cleaned DataFrame to JSON format\n",
    "df_target_cleaned_json = df_target_cleaned.to_dict(orient='records')\n",
    "\n",
    "# Save the JSON to a file\n",
    "with open('property_data_unsold_predictions.json', 'w') as json_file:\n",
    "    json.dump(df_target_cleaned_json, json_file, indent=4)\n",
    "\n",
    "# Display a few entries from the JSON file\n",
    "df_target_cleaned_json[:5]\n",
    "\n",
    "# Notify desktop that the notebook is complete\n",
    "notify_when_done('Jupyter', 'Notebook execution has completed final cell.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
